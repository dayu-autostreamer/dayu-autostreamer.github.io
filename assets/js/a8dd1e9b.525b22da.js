"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7087],{1657:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q24-2-b2316d2cf4ee1288f5324df9a0da9876.png"},3257:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q12-1-60f1a6a94ca2026686a35355f721293d.png"},4594:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q17-669c6d2f79f3164af4465da5421337b5.png"},5481:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q9-3-591f4e53eb2b30e79c052d9cc8a342d2.png"},5584:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q9-2-7da49098d112b131753b39531fdc55aa.png"},6514:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q24-1-b02734af65fce88ec4459259d817bc36.png"},6832:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q24-3-af894ddc03e47d6514186c0bb7f079ba.png"},6875:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q9-1-7d0151af41d7e0fcc09e05810fef0580.png"},7319:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"getting-started/install-lower-layer-system/faqs","title":"FAQs","description":"If you encounter errors in dayu system, please follow these steps to troubleshoot and use FAQs to solve:","source":"@site/docs/03-getting-started/02-install-lower-layer-system/08-faqs.mdx","sourceDirName":"03-getting-started/02-install-lower-layer-system","slug":"/getting-started/install-lower-layer-system/faqs","permalink":"/docs/getting-started/install-lower-layer-system/faqs","draft":false,"unlisted":false,"editUrl":null,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_label":"FAQs","slug":"/getting-started/install-lower-layer-system/faqs","custom_edit_url":null},"sidebar":"tutorialSidebar","previous":{"title":"Common Commands","permalink":"/docs/getting-started/install-lower-layer-system/common-commands"},"next":{"title":"Start Upper Layer System","permalink":"/docs/getting-started/start-upper-layer-system"}}');var o=s(4848),i=s(8453);const r={sidebar_label:"FAQs",slug:"/getting-started/install-lower-layer-system/faqs",custom_edit_url:null},c="FAQs",d={},a=[{value:"Question 1: kube-proxy report iptables problems",id:"question-1-kube-proxy-report-iptables-problems",level:2},{value:"Question 2: calico and coredns are always in initializing state",id:"question-2-calico-and-coredns-are-always-in-initializing-state",level:2},{value:"Question 3: metrics-server keeps unsuccessful state",id:"question-3-metrics-server-keeps-unsuccessful-state",level:2},{value:"Question 4: 10002 already in use",id:"question-4-10002-already-in-use",level:2},{value:"Question 5: edgecore file exists",id:"question-5-edgecore-file-exists",level:2},{value:"Question 6: TLSStreamPrivateKeyFile not exist",id:"question-6-tlsstreamprivatekeyfile-not-exist",level:2},{value:"Question 7: EdgeMesh has successful edge-edge but failed cloud-edge connection",id:"question-7-edgemesh-has-successful-edge-edge-but-failed-cloud-edge-connection",level:2},{value:"Question 8: GPU is not found on master",id:"question-8-gpu-is-not-found-on-master",level:2},{value:"Question 9: cannot find GPU resources in jetson",id:"question-9-cannot-find-gpu-resources-in-jetson",level:2},{value:"Question 10: lc127.0.0. 53:53 no such host/connection refused",id:"question-10-lc12700-5353-no-such-hostconnection-refused",level:2},{value:"Question 11: 169.254.96.16 such host",id:"question-11-1692549616-such-host",level:2},{value:"Question 12: <code>kubectl logs &lt;pod-name&gt;</code> timeout",id:"question-12-kubectl-logs-pod-name-timeout",level:2},{value:"Question 13: Stuck in <code>kubectl logs &lt;pod-name&gt;</code>",id:"question-13-stuck-in-kubectl-logs-pod-name",level:2},{value:"Question 14: CloudCore reports certficate error",id:"question-14-cloudcore-reports-certficate-error",level:2},{value:"Question 15: deleting namespace stucks in terminating state",id:"question-15-deleting-namespace-stucks-in-terminating-state",level:2},{value:"Question 16: Deployment failed after forcibly deleting pod",id:"question-16-deployment-failed-after-forcibly-deleting-pod",level:2},{value:"Question 17: After deleting deployments and pods, the pods still restart automatically",id:"question-17-after-deleting-deployments-and-pods-the-pods-still-restart-automatically",level:2},{value:"Question 18: Large-scale Evicted (disk pressure)",id:"question-18-large-scale-evicted-disk-pressure",level:2},{value:"Question 19: Executing &#39;iptables&#39; report that system does not support &#39;--dport&#39;",id:"question-19-executing-iptables-report-that-system-does-not-support---dport",level:2},{value:"Question 20: Report &#39;token format error&#39; after &#39;keadm join&#39;",id:"question-20-report-token-format-error-after-keadm-join",level:2},{value:"Question 21: Report mapping errors after restart edgecore.service",id:"question-21-report-mapping-errors-after-restart-edgecoreservice",level:2},{value:"Question 22: Restart edgecore and find error of &#39;connect refuse&#39;",id:"question-22-restart-edgecore-and-find-error-of-connect-refuse",level:2},{value:"Question 23: Error of &#39;Shutting down&#39; is reported when deploying metrics-service",id:"question-23-error-of-shutting-down-is-reported-when-deploying-metrics-service",level:2},{value:"Question 24: 169.254.96. 16:53: i/o timeout",id:"question-24-16925496-1653-io-timeout",level:2},{value:"Question 25: keadm join error on edge nodes",id:"question-25-keadm-join-error-on-edge-nodes",level:2},{value:"Question 26: Failed to build map of initial containers from runtime",id:"question-26-failed-to-build-map-of-initial-containers-from-runtime",level:2},{value:"Question 27: Certificate has expired or is not yet valid",id:"question-27-certificate-has-expired-or-is-not-yet-valid",level:2}];function l(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"faqs",children:"FAQs"})}),"\n",(0,o.jsxs)(n.admonition,{title:"Debug Tips in Dayu System",type:"tip",children:[(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"If you encounter errors in dayu system, please follow these steps to troubleshoot and use FAQs to solve:"})}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Dayu Pods:"})," Use ",(0,o.jsx)(n.code,{children:"kubectl get pods -n <your-dayu-namespace>"})," to check dayu pods status.\nUse ",(0,o.jsx)(n.code,{children:"kubectl logs <pod-name> -n <your-dayu-namespace>"})," and ",(0,o.jsx)(n.code,{children:"kubectl describes pod <pod-name> -n <your-dayu-namespace>"})," to check log information in abnormal pods."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Dayu Core Resources:"})," Use ",(0,o.jsx)(n.code,{children:"kubectl get svc -n <your-dayu-namespace>"})," and ",(0,o.jsx)(n.code,{children:"kubectl get deployment -n <your-dayu-namespace>"})," to check services and deployments if pods are in unknown status."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Node Connections:"})," Use ",(0,o.jsx)(n.code,{children:"kubectl get nodes"})," to check node status in the distributed cloud-edge system ('Ready' or 'Not Ready')."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Sedna:"})," Check Sedna gm and lc logs with ",(0,o.jsx)(n.code,{children:"kubectl get pods -n sedna -owide"})," and ",(0,o.jsx)(n.code,{children:"kubectl logs <sedna-pod-name> -n sedna"}),"."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check EdgeMesh:"})," Check EdgeMesh agent logs with ",(0,o.jsx)(n.code,{children:"kubectl get pods -n kubeedge -owide"})," and ",(0,o.jsx)(n.code,{children:"kubectl logs <edgemesh-pod-name> -n kubeedge"})," if communication between nodes is blocked."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Cloudcore:"})," Use ",(0,o.jsx)(n.code,{children:"systemctl status cloudcore"})," or ",(0,o.jsx)(n.code,{children:"journalctl -u cloudcore -xe"})," on the cloud server to check cloudcore is running without errors."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Edgecore:"})," Use ",(0,o.jsx)(n.code,{children:"systemctl status edgecore"})," or ",(0,o.jsx)(n.code,{children:"journalctl -u edgecore -xe"})," on the edge device to check edgecore is running without errors."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Kubelet"})," Use ",(0,o.jsx)(n.code,{children:"systemctl status kubelet"})," or ",(0,o.jsx)(n.code,{children:"journalctl -u kubelet -xe"})," on the cloud server to check k8s administrator service is running without errors."]}),(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Check Docker:"})," If none of the ",(0,o.jsx)(n.code,{children:"kubectl"})," commands or other k8s commands works, check docker container status with ",(0,o.jsx)(n.code,{children:"docker ps -a"})," and check docker logs with ",(0,o.jsx)(n.code,{children:"systemctl status docker"})," or ",(0,o.jsx)(n.code,{children:"journalctl -u docker -xe"}),"."]})]}),"\n",(0,o.jsx)(n.h2,{id:"question-1-kube-proxy-report-iptables-problems",children:"Question 1: kube-proxy report iptables problems"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"E0627 09:28:54.054930 1 proxier.go:1598] Failed to execute iptables-restore: exit status 1 (iptables-restore: line 86 failed ) I0627 09:28:54.054962 1 proxier.go:879] Sync failed; retrying in 30s\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Clear iptables directly."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-2-calico-and-coredns-are-always-in-initializing-state",children:"Question 2: calico and coredns are always in initializing state"}),"\n",(0,o.jsxs)(n.p,{children:["The follwoing message will occur when using ",(0,o.jsx)(n.code,{children:"kubectl describe <podname>"}),"  which is roughly related to network and sandbox issues."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container "7f5b66ebecdfc2c206027a2afcb9d1a58ec5db1a6a10a91d4d60c0079236e401" network for pod "calico-kube-controllers-577f77cb5c-99t8z": networkPlugin cni failed to set up pod "calico-kube-controllers-577f77cb5c-99t8z_kube-system" network: error getting ClusterInformation: Get "https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default": dial tcp 10.96.0. 1:443: i/o timeout, failed to clean up sandbox container "7f5b66ebecdfc2c206027a2afcb9d1a58ec5db1a6a10a91d4d60c0079236e401" network for pod "calico-kube-controllers-577f77cb5c-99t8z": networkPlugin cni failed to teardown pod "calico-kube-controllers-577f77cb5c-99t8z_kube-system" network: error getting ClusterInformation: Get "https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default": dial tcp 10.96.0. 1:443: i/o timeout]\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," Such a problem occurs when a k8s cluster is initialized more than once\nand the network configuration of k8s was not deleted previously."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# delete k8s network configuration\nrm -rf /etc/cni/net.d/  \n\n# reinitialize k8s with the instruction\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-3-metrics-server-keeps-unsuccessful-state",children:"Question 3: metrics-server keeps unsuccessful state"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," Master node does not add taint."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# add taint on master node\nkubectl taint nodes --all node-role.kubernetes.io/master node-role.kubernetes.io/master\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-4-10002-already-in-use",children:"Question 4: 10002 already in use"}),"\n",(0,o.jsxs)(n.p,{children:["Error message 'xxx already in use' occurs when using ",(0,o.jsx)(n.code,{children:"journalctl -u cloudcore.service -xe"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," The previous processes were not cleaned up."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Find the process occupying the port and directly kill it."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"lsof -i:xxxx\nkill xxxxx\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-5-edgecore-file-exists",children:"Question 5: edgecore file exists"}),"\n",(0,o.jsx)(n.p,{children:"When attempting to create a symbolic link in installing edgecore, the target path already exists and cannot be created."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"execute keadm command failed:  failed to exec 'bash -c sudo ln /etc/kubeedge/edgecore.service /etc/systemd/system/edgecore.service && sudo systemctl daemon-reload && sudo systemctl enable edgecore && sudo systemctl start edgecore', err: ln: failed to create hard link '/etc/systemd/system/edgecore.service': File exists, err: exit status 1\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," ",(0,o.jsx)(n.code,{children:"edgecore.service"})," already exists in the ",(0,o.jsx)(n.code,{children:"/etc/systemd/system/"})," directory\nif edgecore is installed more than once."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Just delete it."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"sudo rm /etc/systemd/system/edgecore.service\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-6-tlsstreamprivatekeyfile-not-exist",children:"Question 6: TLSStreamPrivateKeyFile not exist"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:' TLSStreamPrivateKeyFile: Invalid value: "/etc/kubeedge/certs/stream.key": TLSStreamPrivateKeyFile not exist\n12\u6708 14 23:02:23 cloud.kubeedge cloudcore[196229]:   TLSStreamCertFile: Invalid value: "/etc/kubeedge/certs/stream.crt": TLSStreamCertFile not exist\n12\u6708 14 23:02:23 cloud.kubeedge cloudcore[196229]:   TLSStreamCAFile: Invalid value: "/etc/kubeedge/ca/streamCA.crt": TLSStreamCAFile not exist\n12\u6708 14 23:02:23 cloud.kubeedge cloudcore[196229]: ]\n\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Check whether directory ",(0,o.jsx)(n.code,{children:"/etc/kubeedge"})," has file ",(0,o.jsx)(n.code,{children:"certgen.sh"})," and run ",(0,o.jsx)(n.code,{children:"bash certgen.sh stream"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"question-7-edgemesh-has-successful-edge-edge-but-failed-cloud-edge-connection",children:"Question 7: EdgeMesh has successful edge-edge but failed cloud-edge connection"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Troubleshooting:"})}),"\n",(0,o.jsxs)(n.p,{children:["First, based on the ",(0,o.jsx)(n.strong,{children:"location model"}),", ensure whether the edgemesh-agent container exists on the ",(0,o.jsx)(n.strong,{children:"visited node"})," and whether it is operating normally."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q7-2",src:s(9058).A+"",width:"1440",height:"520"})}),"\n",(0,o.jsxs)(n.p,{children:["This situation is common since the master node usually has taints, which will evict other pods, thereby causing the deployment failure of edgemesh-agent.\nThis issue can be resolved by ",(0,o.jsx)(n.strong,{children:"removing the node taints"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"If both the visiting node and the visited node's edgemesh-agent have been started normally while this error is still reported,\nit may be due to unsucessful discovering between the visiting node and the visited node. Please troubleshoot in this way:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"edgemesh-agent at each node has a peer ID (generated with the hash of node name):"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"edge1:\nI'm {12D3KooWFz1dKY8L3JC8wAY6sJ5MswvPEGKysPCfcaGxFmeH7wkz: [/ip4/127.0.0.1/tcp/20006 /ip4/192.168.1.2/tcp/20006]}\n\nedge2: \nI'm {12D3KooWPpY4GqqNF3sLC397fMz5ZZfxmtMTNa1gLYFopWbHxZDt: [/ip4/127.0.0.1/tcp/20006 /ip4/192.168.1.4/tcp/20006]}\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["If visiting node and visited node are in the same LAN with internet IP, refer to Question 12 in ",(0,o.jsx)(n.a,{href:"https://zhuanlan.zhihu.com/p/585749690",children:"EdgeMesh Q&A"}),". Logs of discovering node in the same LAN is  ",(0,o.jsx)(n.code,{children:"[MDNS] Discovery found peer: <visited node peer ID: [visited IP list(include relay node IP)]>"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["If visiting node and visited node are across different LANs, check setting of relayNodes (Details at ",(0,o.jsx)(n.a,{href:"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/4whnkMM9oOaWRsI1ICsvSA",children:"KubeEdge EdgeMesh Architecture"}),"). Logs of discovering node across LANs is ",(0,o.jsx)(n.code,{children:"[DHT] Discovery found peer: <visited node peer ID: [visited IP list(include relay node IP)]>"}),"."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsxs)(n.p,{children:["Before deploy EdgeMesh with ",(0,o.jsx)(n.code,{children:"kubectl apply -f build/agent/resources/"}),", modify file ",(0,o.jsx)(n.code,{children:"04-configmap.yaml"})," and add relayNode."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q7",src:s(9791).A+"",width:"888",height:"1081"})}),"\n",(0,o.jsx)(n.h2,{id:"question-8-gpu-is-not-found-on-master",children:"Question 8: GPU is not found on master"}),"\n",(0,o.jsxs)(n.p,{children:["Use ",(0,o.jsx)(n.code,{children:"nvidia-smi"})," to see GPU status on the cloud server. To support GPU in k8s pod on cloud server, extra plugin is needed."]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Please refer to the following link:\n",(0,o.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration",children:"Installing the NVIDIA Container Toolkit \u2014 NVIDIA Container Toolkit 1.14.3 documentation"})]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Configure as the link, and add ",(0,o.jsx)(n.code,{children:"default-runtime"})," in '/etc/docker/daemon.json' (remember to restart docker after modification):"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n    "default-runtime": "nvidia",\n    "runtimes": {\n        "nvidia": {\n            "args": [],\n            "path": "nvidia-container-runtime"\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"question-9-cannot-find-gpu-resources-in-jetson",children:"Question 9: cannot find GPU resources in jetson"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"k8s-device-plugin"})," has been installed successfully, but nvidia pod on jetson nodes (with tegra architecture) reveals that gpu is not found:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"2024/01/04 07:43:58 Retreiving plugins.\n2024/01/04 07:43:58 Detected non-NVML platform: could not load NVML: libnvidia-ml.so.1: cannot open shared object file: No such file or directory\n2024/01/04 07:43:58 Detected non-Tegra platform: /sys/devices/soc0/family file not found\n2024/01/04 07:43:58 Incompatible platform detected\n2024/01/04 07:43:58 If this is a GPU node, did you configure the NVIDIA Container Toolkit?\n2024/01/04 07:43:58 You can check the prerequisites at: https://github.com/NVIDIA/k8s-device-plugin#prerequisites\n2024/01/04 07:43:58 You can learn how to set the runtime at: https://github.com/NVIDIA/k8s-device-plugin#quick-start\n2024/01/04 07:43:58 If this is not a GPU node, you should set up a toleration or nodeSelector to only deploy this plugin on GPU nodes\n2024/01/04 07:43:58 No devices found. Waiting indefinitely.\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q9-1",src:s(6875).A+"",width:"1624",height:"203"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"dpkg -l '*nvidia*'\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q9-2",src:s(5584).A+"",width:"1623",height:"795"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q9-3",src:s(5481).A+"",width:"1447",height:"386"})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://github.com/NVIDIA/k8s-device-plugin/issues/377",children:"Plug in does not detect Tegra device Jetson Nano \xb7 Issue #377 \xb7 NVIDIA/k8s-device-plugin (github.com)"})}),"\n",(0,o.jsxs)(n.p,{children:["Note that looking at the initial logs that you provided you may have been using ",(0,o.jsx)(n.code,{children:"v1.7.0"})," of the NVIDIA Container Toolkit. This is quite an old version and we greatly improved our support for Tegra-based systems with the ",(0,o.jsx)(n.code,{children:"v1.10.0"})," release. It should also be noted that in order to use the GPU Device Plugin on Tegra-based systems (specifically targetting the integrated GPUs) at least ",(0,o.jsx)(n.code,{children:"v1.11.0"})," of the NVIDIA Container Toolkit is required."]}),"\n",(0,o.jsxs)(n.p,{children:["There are no Tegra-specific changes in the ",(0,o.jsx)(n.code,{children:"v1.12.0"})," release, so using the ",(0,o.jsx)(n.code,{children:"v1.11.0"})," release should be sufficient in this case."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Upgrade the ",(0,o.jsx)(n.strong,{children:"NVIDIA Container Toolkit"})," on jetson nodes."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\nsudo apt-get update\n\nsudo apt-get install -y nvidia-container-toolkit\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-10-lc12700-5353-no-such-hostconnection-refused",children:"Question 10: lc127.0.0. 53:53 no such host/connection refused"}),"\n",(0,o.jsxs)(n.p,{children:["During the Sedna installation stage, an error occurs in logs: ",(0,o.jsx)(n.code,{children:"lc127.0.0. 53:53 no such host/connection refused"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," See question 5 in ",(0,o.jsx)(n.a,{href:"https://zhuanlan.zhihu.com/p/585749690",children:"EdgeMesh Q&A"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsxs)(n.p,{children:["First check whether ",(0,o.jsx)(n.code,{children:"Hostnetwork"})," is in the script ",(0,o.jsx)(n.code,{children:"install.sh"}),". Delete ",(0,o.jsx)(n.code,{children:"Hostnetwork"})," options if it exists (",(0,o.jsx)(n.code,{children:"install.sh"})," in ",(0,o.jsx)(n.a,{href:"https://github.com/dayu-autostreamer/dayu-sedna",children:"dayu-cusomized sedna"})," is correct)."]}),"\n",(0,o.jsxs)(n.p,{children:["If ",(0,o.jsx)(n.code,{children:"Hostnetwork"})," does not exist but the error is still reported, use the following method to temporarily solve (",(0,o.jsx)(n.strong,{children:"not recommend"}),"):"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Add ",(0,o.jsx)(n.code,{children:"nameserver 169.254.96.16"})," to the last line of file ",(0,o.jsx)(n.code,{children:"/etc/resolv.conf"})," on edge nodes with sedna pod error."]}),"\n",(0,o.jsxs)(n.li,{children:["Check whether clusterDNS is ",(0,o.jsx)(n.code,{children:"169.254.96.16"})," in ",(0,o.jsx)(n.code,{children:"/etc/kubeedge/config/edgecore.yaml"})," on edge nodes."]}),"\n",(0,o.jsx)(n.li,{children:"If error still exists, reinstall Sedna."}),"\n"]}),"\n",(0,o.jsxs)(n.h2,{id:"question-11-1692549616-such-host",children:["Question 11: 169.254.96.16",":no"," such host"]}),"\n",(0,o.jsx)(n.p,{children:"Check the configuration of EdgeMesh:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Check the chain in iptables as ",(0,o.jsx)(n.a,{href:"https://edgemesh.netlify.app/zh/advanced/hybird-proxy.html",children:"Hybrid proxy | EdgeMesh"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["Check ",(0,o.jsx)(n.code,{children:"clusterDNS"})," as required in ",(0,o.jsx)(n.a,{href:"/docs/getting-started/install-lower-layer-system/install-edgemesh#configure-edge-network-edge",children:"EdgeMesh Installation"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.h2,{id:"question-12-kubectl-logs-pod-name-timeout",children:["Question 12: ",(0,o.jsx)(n.code,{children:"kubectl logs <pod-name>"})," timeout"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q12-1",src:s(3257).A+"",width:"1618",height:"42"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," Refer to ",(0,o.jsx)(n.a,{href:"https://zhuanlan.zhihu.com/p/379962934",children:"Kubernetes nodes cannot capture monitoring metrics(zhihu.com)"})]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q12-2",src:s(8114).A+"",width:"896",height:"428"})}),"\n",(0,o.jsxs)(n.p,{children:["It can be found that the error happened in port ",(0,o.jsx)(n.code,{children:"10350"}),", which is responsible for forwarding in kubeedge.\nTherefore, it should be a configuration issue with ",(0,o.jsx)(n.code,{children:"cloudcore"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Please set as ",(0,o.jsx)(n.a,{href:"https://kubeedge.io/docs/advanced/debug/",children:"Enable Kubectl logs/exec to debug pods on the edge | KubeEdge"}),"."]}),"\n",(0,o.jsxs)(n.h2,{id:"question-13-stuck-in-kubectl-logs-pod-name",children:["Question 13: Stuck in ",(0,o.jsx)(n.code,{children:"kubectl logs <pod-name>"})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," It maybe caused by the force termination of ",(0,o.jsx)(n.code,{children:"kubectl logs"})," command."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Restart edgecore/cloudcore:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# on cloud\nsystemctl restart cloudcore.service\n# on edge\nsystemctl restart edgecore.service\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-14-cloudcore-reports-certficate-error",children:"Question 14: CloudCore reports certficate error"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q14",src:s(9737).A+"",width:"1628",height:"310"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," Token of master has changed."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Use new token from cloud to join."]}),"\n",(0,o.jsx)(n.h2,{id:"question-15-deleting-namespace-stucks-in-terminating-state",children:"Question 15: deleting namespace stucks in terminating state"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Method 1"})," (Probably won't work)"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl delete ns sedna --force --grace-period=0\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Method 2"}),"\uff1a"]}),"\n",(0,o.jsx)(n.p,{children:"Open a proxy terminal:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl proxy\n\nStarting to serve on 127.0.0.1:8001\n"})}),"\n",(0,o.jsx)(n.p,{children:"Open an operating terminal:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# save configuration file of namespace (use sedna as example here)\nkubectl get ns sedna -o json > sedna.json\n# Delete the contents of spec and status sections and the "," sign after the metadata field.\n'})}),"\n",(0,o.jsxs)(n.p,{children:["The remaining content in ",(0,o.jsx)(n.code,{children:"sedna.json"})," is shown as follows:"]}),"\n",(0,o.jsx)(t,{children:(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n    "apiVersion": "v1",\n    "kind": "Namespace",\n    "metadata": {\n        "creationTimestamp": "2023-12-14T09:12:13Z",\n        "deletionTimestamp": "2023-12-14T09:15:25Z",\n        "managedFields": [\n            {\n                "apiVersion": "v1",\n                "fieldsType": "FieldsV1",\n                "fieldsV1": {\n                    "f:status": {\n                        "f:phase": {}\n                    }\n                },\n                "manager": "kubectl-create",\n                "operation": "Update",\n                "time": "2023-12-14T09:12:13Z"\n            },\n            {\n                "apiVersion": "v1",\n                "fieldsType": "FieldsV1",\n                "fieldsV1": {\n                    "f:status": {\n                        "f:conditions": {\n                            ".": {},\n                            "k:{\\"type\\":\\"NamespaceContentRemaining\\"}": {\n                                ".": {},\n                                "f:lastTransitionTime": {},\n                                "f:message": {},\n                                "f:reason": {},\n                                "f:status": {},\n                                "f:type": {}\n                            },\n                            "k:{\\"type\\":\\"NamespaceDeletionContentFailure\\"}": {\n                                ".": {},\n                                "f:lastTransitionTime": {},\n                                "f:message": {},\n                                "f:reason": {},\n                                "f:status": {},\n                                "f:type": {}\n                            },\n                            "k:{\\"type\\":\\"NamespaceDeletionDiscoveryFailure\\"}": {\n                                ".": {},\n                                "f:lastTransitionTime": {},\n                                "f:message": {},\n                                "f:reason": {},\n                                "f:status": {},\n                                "f:type": {}\n                            },\n                            "k:{\\"type\\":\\"NamespaceDeletionGroupVersionParsingFailure\\"}": {\n                                ".": {},\n                                "f:lastTransitionTime": {},\n                                "f:message": {},\n                                "f:reason": {},\n                                "f:status": {},\n                                "f:type": {}\n                            },\n                            "k:{\\"type\\":\\"NamespaceFinalizersRemaining\\"}": {\n                                ".": {},\n                                "f:lastTransitionTime": {},\n                                "f:message": {},\n                                "f:reason": {},\n                                "f:status": {},\n                                "f:type": {}\n                            }\n                        }\n                    }\n                },\n                "manager": "kube-controller-manager",\n                "operation": "Update",\n                "time": "2023-12-14T09:15:30Z"\n            }\n        ],\n        "name": "sedna",\n        "resourceVersion": "3351515",\n        "uid": "99cb8afb-a4c1-45e6-960d-ff1b4894773d"\n    }\n}\n'})})}),"\n",(0,o.jsx)(n.p,{children:"Delete namespace by call the interface:"}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl -k -H "Content-Type: application/json" -X PUT --data-binary @sedna.json http://127.0.0.1:8001/api/v1/namespaces/sedna/finalize\n'})})}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'{\n  "kind": "Namespace",\n  "apiVersion": "v1",\n  "metadata": {\n    "name": "sedna",\n    "uid": "99cb8afb-a4c1-45e6-960d-ff1b4894773d",\n    "resourceVersion": "3351515",\n    "creationTimestamp": "2023-12-14T09:12:13Z",\n    "deletionTimestamp": "2023-12-14T09:15:25Z",\n    "managedFields": [\n      {\n        "manager": "curl",\n        "operation": "Update",\n        "apiVersion": "v1",\n        "time": "2023-12-14T09:42:38Z",\n        "fieldsType": "FieldsV1",\n        "fieldsV1": {"f:status":{"f:phase":{}}}\n      }\n    ]\n  },\n  "spec": {\n\n  },\n  "status": {\n    "phase": "Terminating",\n    "conditions": [\n      {\n        "type": "NamespaceDeletionDiscoveryFailure",\n        "status": "True",\n        "lastTransitionTime": "2023-12-14T09:15:30Z",\n        "reason": "DiscoveryFailed",\n        "message": "Discovery failed for some groups, 1 failing: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request"\n      },\n      {\n        "type": "NamespaceDeletionGroupVersionParsingFailure",\n        "status": "False",\n        "lastTransitionTime": "2023-12-14T09:15:30Z",\n        "reason": "ParsedGroupVersions",\n        "message": "All legacy kube types successfully parsed"\n      },\n      {\n        "type": "NamespaceDeletionContentFailure",\n        "status": "False",\n        "lastTransitionTime": "2023-12-14T09:15:30Z",\n        "reason": "ContentDeleted",\n        "message": "All content successfully deleted, may be waiting on finalization"\n      },\n      {\n        "type": "NamespaceContentRemaining",\n        "status": "False",\n        "lastTransitionTime": "2023-12-14T09:15:30Z",\n        "reason": "ContentRemoved",\n        "message": "All content successfully removed"\n      },\n      {\n        "type": "NamespaceFinalizersRemaining",\n        "status": "False",\n        "lastTransitionTime": "2023-12-14T09:15:30Z",\n        "reason": "ContentHasNoFinalizers",\n        "message": "All content-preserving finalizers finished"\n      }\n    ]\n  }\n}\n'})})]}),"\n",(0,o.jsx)(n.h2,{id:"question-16-deployment-failed-after-forcibly-deleting-pod",children:"Question 16: Deployment failed after forcibly deleting pod"}),"\n",(0,o.jsxs)(n.p,{children:["Deployment creates pods. But after deleting deployment through ",(0,o.jsx)(n.code,{children:"kubectl delete deploy <deploy-name>"}),", pods are stuck in 'terminating' status."]}),"\n",(0,o.jsxs)(n.p,{children:["Then, use ",(0,o.jsx)(n.code,{children:"kubectl delete pod edgeworker-deployment-7g5hs-58dffc5cd7-b77wz --force --grace-period=0"})," to delete pods. And the re-deployed pods are stuck in 'pending' state after assigning to node."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsxs)(n.p,{children:["Deleting pods with option ",(0,o.jsx)(n.code,{children:"--force"})," does not actually terminate pods."]}),"\n",(0,o.jsxs)(n.p,{children:["You should go to the specific node and manually delete the corresponding docker containers (including pause, see details at ",(0,o.jsx)(n.a,{href:"https://zhuanlan.zhihu.com/p/464712164",children:"Pause in K8S Pod"}),")"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Check pod position on cloud\nkubectl get pods -A -owide\n# Delete docker containers on specific node (the position of pod)\ndocker ps -A\ndocker stop <container>\ndocker rm <container>\n# Restart edgecore on specific node (the pod)\nsystemctl restart edgecore.service\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-17-after-deleting-deployments-and-pods-the-pods-still-restart-automatically",children:"Question 17: After deleting deployments and pods, the pods still restart automatically"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:" journalctl -u edgecore.service  -f\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q17",src:s(4594).A+"",width:"1557",height:"148"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Restart ",(0,o.jsx)(n.code,{children:"edgecore"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"systemctl restart edgecore.service\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-18-large-scale-evicted-disk-pressure",children:"Question 18: Large-scale Evicted (disk pressure)"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Reason:"})}),"\n",(0,o.jsxs)(n.p,{children:["The free disk space (usually root directory) is below the threshold (15%), and kubelet will kill pods to recycle resources (Details at ",(0,o.jsx)(n.a,{href:"https://links.jianshu.com/go?to=https%3A%2F%2Fkubernetes.io%2Fdocs%2Ftasks%2Fadminister-cluster%2Fout-of-resource%2F",children:"K8S Insufficient Resource"}),")."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsx)(n.p,{children:"The fundamental solution is to increase the available disk space."}),"\n",(0,o.jsx)(n.p,{children:"If a temporary solution is needed, the threshold can be changed by modifying the k8s configuration."}),"\n",(0,o.jsxs)(n.p,{children:["Specifically, modify the configuration file and add ",(0,o.jsx)(n.code,{children:"--eviction-hard=nodefs.available<5%"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"systemctl status kubelet\n\n\u25cf kubelet.service - kubelet: The Kubernetes Node Agent\n     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)\n    Drop-In: /etc/systemd/system/kubelet.service.d\n             \u2514\u250010-kubeadm.conf\n     Active: active (running) since Fri 2023-12-15 09:17:50 CST; 5min ago\n       Docs: https://kubernetes.io/docs/home/\n   Main PID: 1070209 (kubelet)\n      Tasks: 59 (limit: 309024)\n     Memory: 67.2M\n     CGroup: /system.slice/kubelet.service\n"})}),"\n",(0,o.jsxs)(n.p,{children:["From the outputs you can see that configuration file is at directory ",(0,o.jsx)(n.code,{children:"/etc/systemd/system/kubelet.service.d"}),"\nand configuration file is ",(0,o.jsx)(n.code,{children:"10-kubeadm.conf"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n\n[Service]\nEnvironment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"\nEnvironment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --eviction-hard=nodefs.available<5%"\n\n# add \'--eviction-hard=nodefs.available<5%\' at the end\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Restart ",(0,o.jsx)(n.code,{children:"kubelet"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"systemctl daemon-reload\nsystemctl  restart kubelet\n"})}),"\n",(0,o.jsx)(n.p,{children:"You will find that it can be deployed normally (it's just a temporary method, the disk space needs to be cleaned up)."}),"\n",(0,o.jsx)(n.h2,{id:"question-19-executing-iptables-report-that-system-does-not-support---dport",children:"Question 19: Executing 'iptables' report that system does not support '--dport'"}),"\n",(0,o.jsxs)(n.p,{children:["When executing command ",(0,o.jsx)(n.code,{children:"iptables -t nat -A OUTPUT -p tcp --dport 10351 -j DNAT --to $CLOUDCOREIPS:10003"}),", an error of not supporting ",(0,o.jsx)(n.code,{children:"--dport"})," is reported."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," The ",(0,o.jsx)(n.code,{children:"iptables"})," version not support ",(0,o.jsx)(n.code,{children:"--dport"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Check iptables version\niptables -V\n# Version of 'iptables v1.8.7 (nf_tables)' does not support '--dport'\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Switch version. In the three options, choose 'legacy'\nsudo update-alternatives --config iptables\n# verify (success with no error)\niptables -t nat -A OUTPUT -p tcp --dport 10351 -j DNAT --to $CLOUDCOREIPS:10003\n"})}),"\n",(0,o.jsx)(n.h2,{id:"question-20-report-token-format-error-after-keadm-join",children:"Question 20: Report 'token format error' after 'keadm join'"}),"\n",(0,o.jsxs)(n.p,{children:["After executing command ",(0,o.jsx)(n.code,{children:"keadm join --cloudcore-ipport=114.212.81.11:10000 --kubeedge-version=1.9.2 --token=\u2026\u2026"}),", ",(0,o.jsx)(n.code,{children:"journalctl -u edegecore.service -f"}),"report error of token format."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsxs)(n.p,{children:["Token is error or expired when joining. Therefore, get the latest token from cloud and redo from ",(0,o.jsx)(n.code,{children:"keadm reset"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"question-21-report-mapping-errors-after-restart-edgecoreservice",children:"Question 21: Report mapping errors after restart edgecore.service"}),"\n",(0,o.jsxs)(n.p,{children:["After executing command ",(0,o.jsx)(n.code,{children:"systemctl restart edgecore.service"})," to restart edgecore, ",(0,o.jsx)(n.code,{children:"journalctl -u edegecore.service -f"})," report mapping error of failing to translate yaml to json."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsxs)(n.p,{children:["Check the format in file",(0,o.jsx)(n.code,{children:"/etc/kubeedge/config/edgecore.yaml"}),". Note that tab is not allowed in YAML files (use space instead)."]}),"\n",(0,o.jsx)(n.h2,{id:"question-22-restart-edgecore-and-find-error-of-connect-refuse",children:"Question 22: Restart edgecore and find error of 'connect refuse'"}),"\n",(0,o.jsxs)(n.p,{children:["During the startup of EdgeMesh, modify the ",(0,o.jsx)(n.code,{children:"/etc/kubeedge/config/edgecore.yaml"})," file and restart the service with ",(0,o.jsx)(n.code,{children:"systemctl restart edgecore.service"}),".\nCommand ",(0,o.jsx)(n.code,{children:"journalctl -u edegecore.service -f"})," report an error of 'connect refuse' and the cloud denies communication."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsxs)(n.p,{children:["Check the status of ",(0,o.jsx)(n.code,{children:"cloudcore"})," on cloud:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# check cloudcore status\nsystemctl status cloudcore\n# restart cloudcore\nsystemctl restart cloudcore.service\n# check error message\njournalctl -u cloudcore.service -f\n"})}),"\n",(0,o.jsxs)(n.p,{children:["An error of ",(0,o.jsx)(n.a,{href:"#question-4-10002-already-in-use",children:"Question 4"})," maybe found."]}),"\n",(0,o.jsx)(n.h2,{id:"question-23-error-of-shutting-down-is-reported-when-deploying-metrics-service",children:"Question 23: Error of 'Shutting down' is reported when deploying metrics-service"}),"\n",(0,o.jsxs)(n.p,{children:["During the deployment of metrics-service, the port in the ",(0,o.jsx)(n.code,{children:"components.yaml"})," file is modified to '4443', but metrics-service still failed with error ",(0,o.jsx)(n.code,{children:"Shutting down RequestHeaderAuthRequestController occurred"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsxs)(n.p,{children:["When deploying KubeEdge, the port in the metrics-service will be automatically overwritten to '10250'. Manually modify the port in ",(0,o.jsx)(n.code,{children:"components.yaml"})," file to '10250'."]}),"\n",(0,o.jsx)(n.h2,{id:"question-24-16925496-1653-io-timeout",children:"Question 24: 169.254.96. 16:53: i/o timeout"}),"\n",(0,o.jsxs)(n.p,{children:["When a new node joins in the cluster, Sedna will be deployed automatically.\nAn error is reported in the log of ",(0,o.jsx)(n.code,{children:"sedna-lc"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"client tries to connect global manager(address: gm.sedna:9000) failed, error: dial tcp: lookup gm.sedna on 169.254.96.16:53: read udp 172.17.0.3:49991->169.254.96.16:53: i/o timeout\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Troubleshooting:"})}),"\n",(0,o.jsxs)(n.p,{children:["First check status of ",(0,o.jsx)(n.code,{children:"edgemesh-agent"})," on the new node. The error is reported from ",(0,o.jsx)(n.code,{children:"edgemesh-agent"}),":"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q24-1",src:s(6514).A+"",width:"1009",height:"163"})}),"\n",(0,o.jsxs)(n.p,{children:["Use ",(0,o.jsx)(n.code,{children:"kubectl describe pod <pod-name>"})," and find that assigning to new node is the latest event:"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q24-2",src:s(1657).A+"",width:"1082",height:"275"})}),"\n",(0,o.jsxs)(n.p,{children:["Use ",(0,o.jsx)(n.code,{children:"journalctl -u edgecore.service -xe"})," on the new node to check errors:"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q24-3",src:s(6832).A+"",width:"1544",height:"127"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reason:"})," The new node cannot access dockerhub directly and has not been configured of docker registry. Thus, docker image cannot be pulled."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution:"})," Configure docker registry and restart docker and edgecore. Refer to ",(0,o.jsx)(n.a,{href:"/docs/developer-guide/how-to-build/docker-registry/",children:"Docker Registry Configuration"})," for details."]}),"\n",(0,o.jsx)(n.h2,{id:"question-25-keadm-join-error-on-edge-nodes",children:"Question 25: keadm join error on edge nodes"}),"\n",(0,o.jsxs)(n.p,{children:["Execution of ",(0,o.jsx)(n.code,{children:"keadm join"})," on edges reported errors."]}),"\n",(0,o.jsxs)(n.p,{children:["Solution: Check the ",(0,o.jsx)(n.code,{children:"edgecore.yaml"})," file:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"vim /etc/kubeedge/config/edgecore.yaml\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Q25",src:s(8469).A+"",width:"684",height:"365"})}),"\n",(0,o.jsxs)(n.p,{children:["Add the address of master node (cloud) in edgeHub/httpServer, such as ",(0,o.jsx)(n.code,{children:"https://114.212.81.11:10002"}),",\ndelete the redundant ':' in websocket/server."]}),"\n",(0,o.jsxs)(n.p,{children:["Re-run the ",(0,o.jsx)(n.code,{children:"keadm join"})," command after modification."]}),"\n",(0,o.jsx)(n.h2,{id:"question-26-failed-to-build-map-of-initial-containers-from-runtime",children:"Question 26: Failed to build map of initial containers from runtime"}),"\n",(0,o.jsxs)(n.p,{children:["Edgecore report error when ",(0,o.jsx)(n.code,{children:"journalctl -u edgecore.service -f"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"initialize module error: failed to build map of initial containers from runtime: no PodsandBox found with Id 'c45ed1592e75e885e119664d777107645a7e7904703c690664691c61a9f79ed3'\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Solution:"})}),"\n",(0,o.jsx)(n.p,{children:"Find the docker ID and delete it:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'docker ps -a --filter "label=io.kubernetes.sandbox.id=c45ed1592e75e885e119664d777107645a7e7904703c690664691c61a9f79ed3"\n# find the related docker ID\ndocker rm <docker ID>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"question-27-certificate-has-expired-or-is-not-yet-valid",children:"Question 27: Certificate has expired or is not yet valid"}),"\n",(0,o.jsxs)(n.p,{children:["Execution of ",(0,o.jsx)(n.code,{children:"kubectl get pods -A"})," reports the following error:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2025-06-27T22:23:12+08:00 is after 2025-06-27T08:56:50Z\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Reason:"})}),"\n",(0,o.jsx)(n.p,{children:"Check the certificate of Kubernetes on the cloud server:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"sudo kubeadm certs check-expiration\n\n# Get the following results\n[check-expiration] Reading configuration from the cluster...\n[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration\n\nCERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Jun 27, 2025 08:56 UTC   <invalid>                               no      \napiserver                  Jun 27, 2025 08:56 UTC   <invalid>       ca                      no      \napiserver-etcd-client      Jun 27, 2025 08:56 UTC   <invalid>       etcd-ca                 no      \napiserver-kubelet-client   Jun 27, 2025 08:56 UTC   <invalid>       ca                      no      \ncontroller-manager.conf    Jun 27, 2025 08:56 UTC   <invalid>                               no      \netcd-healthcheck-client    Jun 27, 2025 08:56 UTC   <invalid>       etcd-ca                 no      \netcd-peer                  Jun 27, 2025 08:56 UTC   <invalid>       etcd-ca                 no      \netcd-server                Jun 27, 2025 08:56 UTC   <invalid>       etcd-ca                 no      \nfront-proxy-client         Jun 27, 2025 08:56 UTC   <invalid>       front-proxy-ca          no      \nscheduler.conf             Jun 27, 2025 08:56 UTC   <invalid>                               no      \n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Jun 25, 2034 08:56 UTC   8y              no      \netcd-ca                 Jun 25, 2034 08:56 UTC   8y              no      \nfront-proxy-ca          Jun 25, 2034 08:56 UTC   8y              no      \n"})}),"\n",(0,o.jsxs)(n.p,{children:["The certificates are in status of ",(0,o.jsx)(n.code,{children:"<invalid>"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,o.jsx)(n.p,{children:"Update Kubernetes certificates:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"sudo kubeadm certs renew all\n\n# Get the following results:\n[renew] Reading configuration from the cluster...\n[renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[renew] Error reading configuration from the Cluster. Falling back to default configuration\n\ncertificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed\ncertificate for serving the Kubernetes API renewed\ncertificate the apiserver uses to access etcd renewed\ncertificate for the API server to connect to kubelet renewed\ncertificate embedded in the kubeconfig file for the controller manager to use renewed\ncertificate for liveness probes to healthcheck etcd renewed\ncertificate for etcd nodes to communicate with each other renewed\ncertificate for serving etcd renewed\ncertificate for the front proxy client renewed\ncertificate embedded in the kubeconfig file for the scheduler manager to use renewed\n\nDone renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.\n"})}),"\n",(0,o.jsx)(n.p,{children:"Recheck the certificates, all certificates are updated:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"sudo kubeadm certs check-expiration\n\n# Get the following results:\n[check-expiration] Reading configuration from the cluster...\n[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration\n\nCERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Jun 27, 2026 14:30 UTC   364d                                    no      \napiserver                  Jun 27, 2026 14:30 UTC   364d            ca                      no      \napiserver-etcd-client      Jun 27, 2026 14:30 UTC   364d            etcd-ca                 no      \napiserver-kubelet-client   Jun 27, 2026 14:30 UTC   364d            ca                      no      \ncontroller-manager.conf    Jun 27, 2026 14:30 UTC   364d                                    no      \netcd-healthcheck-client    Jun 27, 2026 14:30 UTC   364d            etcd-ca                 no      \netcd-peer                  Jun 27, 2026 14:30 UTC   364d            etcd-ca                 no      \netcd-server                Jun 27, 2026 14:30 UTC   364d            etcd-ca                 no      \nfront-proxy-client         Jun 27, 2026 14:30 UTC   364d            front-proxy-ca          no      \nscheduler.conf             Jun 27, 2026 14:30 UTC   364d                                    no      \n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Jun 25, 2034 08:56 UTC   8y              no      \netcd-ca                 Jun 25, 2034 08:56 UTC   8y              no      \nfront-proxy-ca          Jun 25, 2034 08:56 UTC   8y              no      \n"})}),"\n",(0,o.jsxs)(n.p,{children:["However, the execution of command  ",(0,o.jsx)(n.code,{children:"kubectl get pods -A"})," still reports errors:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"error: You must be logged in to the server (Unauthorized)\n"})}),"\n",(0,o.jsx)(n.p,{children:"Renew the configuration file and restart core k8s services to solve:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Backup configuration file\ncp -rp $HOME/.kube/config $HOME/.kube/config.bak\n\n# Renew configuration file\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo cp -i /etc/kubernetes/admin.conf /root/.kube/config\n\n# Restart kubelet\nsudo systemctl restart kubelet\n# Restart kube-apiserver, kube-controller-manage, kube-scheduler\ndocker ps |grep kube-apiserver|grep -v pause|awk '{print $1}'|xargs -i docker restart {}\ndocker ps |grep kube-controller-manage|grep -v pause|awk '{print $1}'|xargs -i docker restart {}\ndocker ps |grep kube-scheduler|grep -v pause|awk '{print $1}'|xargs -i docker restart {}\n"})}),"\n",(0,o.jsx)(n.p,{children:"Restart Cloudcore and check the status:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Restart Cloudcore\nsudo systemctl restart cloudcore\n# Check Cloudcore status\nsystemctl status cloudcore\njournalctl -u cloudcore -xe\n"})}),"\n",(0,o.jsx)(n.p,{children:"If Cloudcore has the following errors:"}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cloudcore.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\n"})})}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"-- A start job for unit cloudcore.service has finished successfully.\n-- \n-- The job identifier is 2357999.\n6\u6708 30 00:45:15 cloud.kubeedge cloudcore[3159196]: W0630 00:45:15.125171 3159196 validation.go:154] TLSTunnelPrivateKeyFile does not exist i>\n6\u6708 30 00:45:15 cloud.kubeedge cloudcore[3159196]: W0630 00:45:15.125269 3159196 validation.go:157] TLSTunnelCertFile does not exist in /etc>\n6\u6708 30 00:45:15 cloud.kubeedge cloudcore[3159196]: W0630 00:45:15.125286 3159196 validation.go:160] TLSTunnelCAFile does not exist in /etc/k>\n6\u6708 30 00:45:15 cloud.kubeedge cloudcore[3159196]: I0630 00:45:15.125336 3159196 server.go:77] Version: v1.9.2\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]: panic: failed to create system namespace\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]: goroutine 1 [running]:\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]: github.com/kubeedge/kubeedge/cloud/cmd/cloudcore/app.NewCloudCoreCommand.func1(0xc00035f8>\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]:         /root/kubeedge/cloud/cmd/cloudcore/app/server.go:85 +0x81c\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]: github.com/spf13/cobra.(*Command).execute(0xc00035f8c0, 0xc00003c1f0, 0x0, 0x0, 0xc00035f>\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]:         /root/kubeedge/vendor/github.com/spf13/cobra/command.go:854 +0x2c2\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]: github.com/spf13/cobra.(*Command).ExecuteC(0xc00035f8c0, 0xc000180058, 0x1859280, 0x0)\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]:         /root/kubeedge/vendor/github.com/spf13/cobra/command.go:958 +0x375\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]: github.com/spf13/cobra.(*Command).Execute(...)\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]:         /root/kubeedge/vendor/github.com/spf13/cobra/command.go:895\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]: main.main()\n6\u6708 30 00:45:16 cloud.kubeedge cloudcore[3159196]:         /root/kubeedge/cloud/cmd/cloudcore/cloudcore.go:16 +0x65\n6\u6708 30 00:45:16 cloud.kubeedge systemd[1]: cloudcore.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\n-- Subject: Unit process exited\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- An ExecStart= process belonging to unit cloudcore.service has exited.\n-- \n-- The process' exit code is 'exited' and its exit status is 2.\n6\u6708 30 00:45:16 cloud.kubeedge systemd[1]: cloudcore.service: Failed with result 'exit-code'.\n-- Subject: Unit failed\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit cloudcore.service has entered the 'failed' state with result 'exit-code'.\n"})})]}),"\n",(0,o.jsx)(n.p,{children:"Try to update Cloudcore certs:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Modify the ip address of '--adverse-address'\nkeadm init --advertise-address=114.212.81.11 --kubeedge-version=1.9.2\n\n# Check Cloudcore status\njournalctl -u cloudcore -xe\n\n# if Cloudcore journal report port connection refused, kill the occupied process\nsudo lsof -i:<port>\nsudo kill -9 <pid>\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Remember to modify ",(0,o.jsx)(n.code,{children:"cloudcore.yaml"})," as ",(0,o.jsx)(n.a,{href:"/docs/getting-started/install-lower-layer-system/install-kubeedge#configure-cloudcore",children:"[Install KubeEdge]"})," shows after ",(0,o.jsx)(n.code,{children:"keadm init"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8114:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q12-2-f993592abf40687d45bb005fd2b6ed1c.png"},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>c});var t=s(6540);const o={},i=t.createContext(o);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(i.Provider,{value:n},e.children)}},8469:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q25-dc68c2dfac16a206d5fd83e3a9d1ba11.png"},9058:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q7-2-3eda05d263f0bcb0b0c4f215dea5feca.png"},9737:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q14-6d45356896d3c46128f949c4ca662ba4.png"},9791:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/Q7-8146db94e6f201c47562da7a76cc6ab9.png"}}]);